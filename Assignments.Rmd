---
title: "Assignments"
output:
  html_document:
    toc: yes
    toc_float: yes
    collapsed: no
    number_sections: no
    toc_depth: 1
  pdf_document:
    toc: yes
    toc_depth: '1'
  word_document:
    toc: yes
    toc_depth: '1'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```
This page will contain all the assignments you submit for the class.


### Instructions for all assignments

1. Download the Assignment1.Rmd file from Canvas. You can use this as a template for writing your answers. It's the same as what you can see on my website in the Assignments tab. Once we're done with this I'll edit the text on the website to include the solutions.

2. On RStudio, open a new R script in RStudio (File > New File > R Script). This is where you can test out your R code. You'll write your R commands and draw plots here.

3. Once you have finalized your code, copy and paste your results into this template (Assignment 1.Rmd). For example, if you produced a plot as the solution to one of the problems, you can copy and paste the R code in R markdown by using the ` ``{r} ``` ` command. Answer the questions in full sentences and Save.

4. Produce a PDF file with your answers. To do this, knit to PDF (use Knit button at the top of RStudio), locate the PDF file in your docs folder (it's in the same folder as the Rproj), and submit that on on Canvas in Assignment 1.

5. Build Website, go to GitHub desktop, commit and push. Now your solutions should be on your website as well.






# Assignment 1

**Collaborators: Sara Whitelaw **

This assignment is due on Canvas on Monday 9/20 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.


### Problem 1 

Install the datasets package on the console below using `install.packages("datasets")`. Now load the library.



```{r}
library(datasets)
#This command loads the data set. 
```

Load the USArrests dataset and rename it `dat`. Note that this dataset comes with R, in the package datasets, so there's no need to load data from your computer. Why is it useful to rename the dataset?

```{r}
dat<-USArrests
#This command renames our data set "USArrests" as dat. Renaming USArrests as dat makes the name shorter and easier to use. 
```

### Problem 2

Use this command to make the state names into a new variable called State. 

```{r, eval=TRUE}
dat$state <- tolower(rownames(USArrests))
#This command renames the state names into a variable called state. 
```

This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case.


List the variables contained in the dataset `USArrests`.

```{r}
names(dat)
```
This command is telling me what the different variables in the USArrests data set are.

### Problem 3 

What type of variable (from the DVB chapter) is `Murder`?

Answer: 'Murder' is a quantitative variable. 

What R Type of variable is it?

Answer:In R, 'Murder' is a numerical variable, which means it is a quantitative variable that is expressed with decimal points. 



### Problem 4

What information is contained in this dataset, in general? What do the numbers mean? 

Answer: This data set contains information about different arrest rates in the United States. It  contains statistics for the arrests in every state for three crimes: rape, assault, and murder. The numbers are the amount of arrests per crime per 100,000 residents in that state. The urban pop variable is the percent of residents living in an urban setting.  


### Problem 5

Draw a histogram of `Murder` with proper labels and title.

```{r}
hist(dat$Murder, main='Murder Arrests per 100,000 residents', xlab="Number of Arrests", ylab="Frequency", xlim=c(0,20), ylim=c(0,20),col="pink",breaks=10)

```
### Problem 6

Please summarize `Murder` quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?

```{r}
summary(dat)
```
The average number of murder arrests per 100,000 residents is 7.788.  The median for this data set is 7.250. Mean is the average value of a data set. Mean is equal to the sum of values divided by the number of values. Mean = sum of values/number of values. 
The Median is the middle value of a number set after all of the values have been put in an ordered list. 
Quartiles are three values that divide the data distribution into even fourths.  In particular, Q1 and Q3 are important because we can use these two values to calculate the IQR, or interquartile range. The middle half of the data lies within the IQR.  

### Problem 7

Repeat the same steps you followed for `Murder`, for the variables `Assault` and `Rape`. Now plot all three histograms together. You can do this by using the command `par(mfrow=c(3,1))` and then plotting each of the three. 

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
hist(dat$Assault, main='Assault Arrests per 100,000 residents', xlab="Number of Arrests", ylab="Frequency", col="yellow", xlim=c(0,400), ylim=c(0,20),)
#The average number of assault arrests per 100,000 residents is 170.8.  The median for this data set is 159.0.  
```
```{r, echo = TRUE, fig.width = 5, fig.height = 8}
#Histogram of Rape Arrests per 100,000 Residents 
hist(dat$Rape, main='Rape Arrests per 100,000 residents', xlab="Number of Arrests", ylab="Frequency", col="sky blue", xlim=c(0,60), ylim=c(0,20))
#The average number of rape arrests per 100,000 residents is 21.23 and the median for this data set is 20.10.  
```
```{r, fig.width = 5, fig.height = 8}
par(mfrow=c(3,1))
hist(dat$Rape, main='Rape Arrests per 100,000 residents', xlab="Number of Arrests", ylab="Frequency", col="sky blue", xlim=c(0,60), ylim=c(0,20))
hist(dat$Assault, main='Assault Arrests per 100,000 residents', xlab="Number of Arrests", ylab="Frequency", col="yellow", xlim=c(0,400), ylim=c(0,20),)
hist(dat$Murder, main='Murder Arrests per 100,000 residents', xlab="Number of Arrests", ylab="Frequency", xlim=c(0,20), ylim=c(0,20),col="pink",breaks=10)
```

What does the command par do, in your own words (you can look this up by asking R `?par`)?

Answer:The command par is used to to set graphical parameters.  In this case, it can be used to put multiple graphs on a single plot. 


What can you learn from plotting the histograms together?

Answer: From plotting the histograms together, I can compare frequency of the arrest rates for these three crimes. I can see that the number of assault arrests per 100,000 residents has a much wider spread than rape arrests and murder arrests.  The number of murder and rape arrests per 100,000 residents has a much smaller spread.  This shows that on average, there are more assault arrests per 100,000 residents in the United States than murder or rape arrests.  Looking at histograms graphed together, we can visually compare how different, or similar, the statistical characteristics, such as mean, median, spread, of the different data sets are.      
  
### Problem 8

In the console below (not in text), type `install.packages("maps")` and press Enter, and then type `install.packages("ggplot2")` and press Enter. This will install the packages so you can load the libraries.
```{r}
#install.packages("ggplot2")
#install.packages("maps")
#'maps' and 'ggplot2' were already downloaded, so I was getting an error message when I had the code in for it.
```
Run this code:

```{r, eval = FALSE, fig.width = 7.5, fig.height = 4}
library(maps) 
library(ggplot2) 

ggplot(dat, aes(map_id=state, fill=Murder)) + 
  geom_map(map=map_data("state")) + 
  expand_limits(x=map_data("state")$long, y=map_data("state")$lat)
```

What does this code do? Explain what each line is doing.

Answer: The library code loads the desired packages.  In this case, it is loading the packages called 'map' and 'ggplot2'.  The combination of the 'map' package and the 'ggplot2' package allows for the display of maps. The next line is giving the code for the map that we want to generate of the United States.  The quoting function aes is for aesthetic mappings and the code that follows is describing what is making up the aesthetic mapping. We are pulling data from the USArrests data set, which was shortened to dat. The fill=murder command is telling R that we want the states to be shaded in according to the"Murder" data set.  The expand_limits code describes the x and y axis by longitude and latitude. 


# Assignment 2
Subtitle: Crim 250: Statistics for the Social Sciences
Name: Michaela Rieser
Date: 09/23/2021

###Problem 1: Load data

Read the data
```{r}
read.csv(file="dat.nsduh.small.csv")
dat2 <- read.csv(file = "dat.nsduh.small.csv")
```

What are the dimensions of the dataset? 
```{r}
names(dat2)
```
The variables are mjaje, cigever, alcever, age2, sexatract, speakenglish, irsex.


### Problem 2: Variables

 Describe the variables in the data set.
```{r}
summary(dat2)
```
All of the variables in our study have been made into quantitative variables. For each of the questions, the response was a numerical answer.  Even for the questions that could have been seen as categorical, the respondents answered with a number.  The responses to each question were coded into a numerical value.For example, one of the variables is sexual attraction, and the responses are "I am only attracted to the opposite sex", "I am mostly attracted to opposite sex", "I am equally attracted to males and females", "I am mostly attracted to the same sex", "I am only attracted to the same sex", and "I am not sure" all correspond to a numerical value.  This is useful because we are able to use quantitative variable techniques to learn more about the sample.  For example, we can calculate the mean and median because our responses have been translated into numbers. 

__What is this data set about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data?__
This data set is just a small sample taken from the 2019 National Survey of Drug Use and Health.  The National Survey on Drug Use and Health (NSDUH) was collected by the US Department of Health and Human Services.  NSDUH is a sample survey of the population of United States citizens aged 12 and older.  The goal of this survey is to generate a large statistical information data base on the use of alcohol, tobacco, drugs, and mental health in the United States.  The survey wants to track trends in substance use and mental illness.  For the purpose of this class, we only are looking at some of the data collected in this survey.  We are focused on looking at the trends and spread of the data collected about the age of first marijuana use, the age when individuals first started smoking cigarettes everyday, and the age of first alcohol use. We also are looking at the data that asks how well the respondent speaks English, their current age, their sexual attraction, and their gender. 


###Problem 3: Age and gender

What is the age distribution of the sample like? Make sure you read the codebook to know what the variable values mean.

The histogram does not directly express the years of age along the x axis.  The ages of the respondents were coded into groups.  For example, a value of 1 for age corresponds to the responded being 12 years old I feel that it is difficult to really determine the distribution of ages, because some numerical values correspond to only a single year of age and others correspond to larger age groups, such as 50-64 year olds.  I think it would be better if the ages were expressed as a normal quantitative data set, or each numerical value corresponded to age groups of the same length (for example, the age range for each was 3 years for all, and not varying).From summarizing the data for AGE2 and producing a histogram, I can see that the average was 13.98 which corresponds to an average respondent age of between 30 and 34 years of age. Respondents were all over the age of 12. 
The histogram allows for the visualization of the spread.  The data appears to be skewed left, but I think that this could just be a result of how the ages were grouped.  Of course the graph would appear to be skewed left when the second half of the age code encompasses ages 22 through 65+.  


__Do you think this age distribution representative of the US population? Why or why  not?__
No, I do not think that this age distribution is representative of the US population because 
there is no representation of the population under 12 years of age.  Although these questions would not be appropriate to ask children, if the sampling was for all ages, then perhaps the age distribution would be representative of the US population.  Also, it would make more sense that there would more people under 25 because not all children reach adulthood. 


__Is the sample balanced in terms of gender? If not, are there more females or males?__
```{r}
counts <- table(dat2$irsex)
barplot(counts, main="Gender Distribution", xlab="Gender", ylab="Frequency", names=c("Male", "Female"), ylim=c(0,300), col="sky blue")
```

```{r}
counts <- table(dat2$irsex)
```

```{r}
pie(counts, labels=c("Male", "Female"), main="Gender Distribution")
```

```{r}
table(dat2$irsex)
```

I used both a bar plot and pie chart to look at the gender distribution.  From both graphs, I can see that the sample is not 50% men, 50% women.  There are more male responses in our sample than women.  However, the difference is only slight.  If there was an equal number of men (1) and women (2) in this study, the mean would be exactly 1.5.  However, the mean is slightly less than 1.5, it is 1.468.  Because the mean is pulled slightly closer to 1 (which is the numerical code for male respondent).Also, I was able to make a table of irsex and see how many respondents answered 1(male) or 2(female), 91 answered male and 80 answered female.

__Use this code to draw a stacked bar plot to view the relationship between sex and age. What can you conclude from this plot?__

From a stacked bar plot, we can compare the age and sex distributions of the respondents simultaneously.  With this bar plot, we can compare how sex is distributed in each age category.  For example, in age category 8, which corresponds to respondent is 19 years old, we can see that there were much more female respondents than male (if there were any male at all).


### Problem 4: Substance use

For which of the three substances included in the data set (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?
```{r}
summary(dat2)
```
To answer this question, I looked at both the minimum age and the average age for the first time trying each of these substances. The youngest that any of these respondents had ever tried alcohol was 5 years old, whereas the youngest that anyone had ever tried cigarettes or marijuana was 10 and 7, respectively.The average age at first alcohol drink was 14.95 or ~15 years old.  The average age at first cigarette was 17.65 years old and the average age of first time using marijuana was 15.99 or ~16. On average, individuals tend to use alcohol earliest.  


###Problem 5: Sexual attraction

What does the distribution of sexual attraction look like? Is this what you expected?
```{r}
counts <- table(dat2$sexatract)
barplot(counts, ylim = c(0,150), main="Sexual Attraction/Orientation Distribution", xlab="Sexual Attraction", names=c("1", "2", "3", "4", " 5", "6", "97", "98","99"))
```
The majority of respondents are straight/only attracted to the opposite sex. For this variable, it is not helpful to look at the mean, because it is skewed due to the code 99 for skipped question.  By looking at both the bar plot and the median, Q1 and Q3, I can see that almost all of the data falls in the opposite sex only code of 1.  This is not what I expected at all.Especially today, in which there is a greater level of acceptance of LGBTQIA+ individuals, I would feel that the distribution would not be so centered around 1, only opposite sex attraction. 


__What is the distribution of sexual attraction by gender?__ 
```{r}
tab.gendersexattract <- table(dat2$irsex, dat2$sexatract)
barplot(tab.gendersexattract,
        main = "Sexual Attraction of Men and Women", ylim=c(0,150), 
        xlab = "Sexual Attraction", ylab = "Frequency",
        legend.text = rownames(tab.gendersexattract),
        beside = FALSE) # Stacked bars (default)
```


###Problem 6: English speaking

What does the distribution of English speaking look like in the sample? Is this what you might expect for a random sample of the US population?
```{r}
counts <- table(dat2$speakengl)
barplot(counts, ylim = c(0,200), main="English Speaking Ability", xlab="Language Code ")
```
To answer this question, I looked at both the median, mean, Q1, Q3, and a bar chart for the data collected on English speaking ability.75% or more of the respondents can speak English very well, looking at Q3. 
```{r}
table(dat2$speakengl)
```
Also, by making a table with the data from speakengl, I am able to see that 161 of the respondents answered that they speak English very well, in comparison to only 8 answering 2(well) and 2 answering 3(not well).  

__Are there more English speaker females or males?__
```{r}
tab.genderspeakengl <- table(dat2$irsex, dat2$speakengl)
barplot(tab.genderspeakengl,
        main = "English Speaking Ability of Men and Women", ylim=c(0,200), 
        xlab = "English Speaking Ability", ylab = "Frequency",
        legend.text = rownames(tab.genderspeakengl),
        beside = FALSE) # Stacked bars (default)
```

```{r}
table(dat2$speakengl, dat2$irsex)
```
Looking at the stacked bar plot for gender and English speaking ability, I can't really tell if there are more men or more women able to speak English.I decided to make a table to see exactly how many men and women answered 1 (speaks English very well).  From this table, I can see that 84 men answered this, whereas only 77 women answered that they can speak English very well.However, there were more male respondents than female, so I decided to calculate percent of men and percent of women can speak English "very well". 96.25% of female respondents speak English very well.  86.6% of male respondents speak English very well.  A greater percent of female respondents are able to speak English very well.  However, just looking at number of respondents, more men speak English very well. 
 

$$\\[2in]$$





# Exam 1 

###Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (fatal-police-shootings-data.csv) onto that folder, and save your Exam 1.Rmd file in the same folder.

c. Download the README.md file. This is the codebook. 

d. Load the data into an R data frame.
```{r}
datexam <- read.csv("fatal-police-shootings-data.csv")
```


###Problem 1 (10 points)

a. Describe the dataset. This is the source: https://github.com/washingtonpost/data-police-shootings . Write two sentences (max.) about this.

__This data set is a record of every fatal police shooting in the United States since January 1, 2015.  The data set gives a unique identification for each victim, the name of the victim, the date of the shooting, the manner of death, whether the victim was armed, the age, gender, and race of the victim, the city and state of the shooting, whether or not the victim had signs of mental illness, the threat level, how/if the victim tried to flee, whether or not a body camera was worn by the officer, the latitude and longitude location, and the accuracy of these coordinates. __

b. How many observations are there in the data frame?
```{r}
names(datexam)
```

__There are 17 different variables in the data frame. 6954 individuals were observed (6954 victims). __

c. Look at the names of the variables in the data frame. Describe what "body_camera", "flee", and "armed" represent, according to the codebook. Again, only write one sentence (max) per variable.
```{r}
summary(datexam)
```

__"body_camera" is a categorical variable that represents whether or not the officer was reported in the news to be wearing a body cam at the time of the incident and whether or not any of the incident was recorded. "flee" is another categorical variable that represents whether or not news reports indicated that the victim was fleeing at the time of shooting and if it was by foot or car.  "armed" is another categorical variable that indicates if the victim was armed. __

d. What are three weapons that you are surprised to find in the "armed" variable? Make a table of the values in "armed" to see the options.
```{r}
table(datexam$armed)
```

__This is a very extensive and interesting list of weapons that the victims were armed with.  Three weapons that I was surprised to find in the armed variable were: air conditioner, railroad spikes, and flashlight.  Another weapon that was shocking was a "toy weapon".   __

###Problem 2 (10 points)

a. Describe the age distribution of the sample. Is this what you would expect to see?
```{r}
hist(datexam$age, main="Distribution of Age of Police Shoooting Victims", xlab="Age", ylab="Frequency", xlim=c(0,100 ), ylim=c(0,1500),col="yellow", breaks=25)
```

__The distribution of the data is skewed fairly right.  According to the data, the youngest victim was 6 years old and the oldest victim was 91 years old.  This is a very wide spread and I was shocked and upset to see that the youngest victim of police shooting was 6 years old.  50% of the victims were between the ages of 27 and 45, according to Q1 and Q3.  The distribution is skewed fairly right, with a concentration of victims between the ages of 27 to 45.  The average age of victims was 37.12 years old.  The median was 35 for this data set. __

b. To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.
```{r}
summary(datexam)
```

__To describe the center of the age distribution, I used the median age.  The median age gives a better description of the center of the age distribution because it is the point where half of the victims are younger than and half of the victims are older than this age.  Mean is the average age of the people but median gives a better description of the true center of the distribution.__

c. Describe the gender distribution of the sample. Do you find this surprising?
```{r}
table(datexam$gender)
counts <- table(datexam$gender)
barplot(counts, main="Gender Distribution of Police Shooting Victims", xlab="Gender", ylab="Frequency", names=c("unknown", "Female","Male"),ylim=c(0,7000), col="sky blue", legend=TRUE)

```

__Because gender is a categorical variable, I used a bar plot to look at the distribution of the same.  There are three categories for gender: female, male, and unknown gender. Looking at the table displaying the counts of male and female victims in union with the bar plot, it is clear that the number of male police shooting victims outweighs the number of female police shooting victims. I am not surprised by this, especially with the recent news and all of the reports I have heard about about male shooting victims.  3 victims had unknown gender, 293 were women, and 6298 were men. __


###Problem 3 (10 points)

a. How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?

```{r}
table(datexam$body_camera)
```

__910 police officers had a body camera. 13.8% of police officers were wearing a body camera and could have recorded a portion of the incidents  I am surprised that this is so low.  You would think that if a police officer was being attacked they would activate their body camera to record the incident.  However, with all of the recent news about the various police shooting incidents, I am not surprised that police officers are not using their body cameras in incidents were the victims were not actually a threat.  __

b. In  how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?
```{r}
table(datexam$flee)
```

__In 1058 incidents, the victim was feeling by car.  In 845 incidents, they were fleeing by foot. In 491, it is unknown whether or not the victim was fleeing.  In total, in 1903 of these incidents, the victim was fleeing before being shot.  In 16.58% of the incidents, the victim was fleeing before the shooting. I honestly cannot decide whether or not this is what I would expect.  __



###Problem 4 (10 points) -  Answer only one of these (a or b).

a. Describe the relationship between the variables "body camera" and "flee" using a stacked barplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the options for "flee", each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).*

*Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}

```

__Your answer here.__

b. Describe the relationship between age and race by using a boxplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the race categories and the height along the y-axis is age.* 

*Hint 2: Also, if you are unsure about the syntax for boxplot, run ?boxplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}
table(datexam$race)
plot(factor(datexam$race), datexam$age, ylab="Age", xlab="Race", names=c("unknown", "Asian", "Black", "Hispanic", "NA", "Other", "White"), main="Relationship between Age and Race of Police Shooting Victims", col="red")
table(datexam$age, datexam$race)
```

__NA stands for Native American on our box plot.  The box for "unknown" is missing or unknown racial data.  We should ignore this in our analysis of age distribution within race.  The box plot shows the distribution of age within each race category.  This box plot breaks the victims into their race categories, and then displays the distribution of age in each category. From the box plot, we visualize the distribution of age in each race more easily and compare the distributions across races.  From this, I can see that the ages of white victims of police shootings has the most spread.  From looking at the table, I can see that the minimum age of white victims was 6 years old and the maximum age was 91.  The range for the other racial categories is not as large.  Looking at the box plots for all of the relationships, I can see that the average age of police shooting victims of for all 6 racial categories is under 40. There are a few age outliers for each race, the most for Black.  The whiskers on the "white" boxplot are much longer than others.  From this boxplot, we can conclude that, regardless of outliers and other discrepancies, the distribution of age of police shooting victims is similar regardless of victim's race. __






###Extra credit (10 points)

a. What does this code tell us? 

```{r, eval=FALSE}
mydates <- as.Date(dat$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])
```

 __The first line is loading the data as calendar dates (d/m/year).  I am not sure about the rest of the code. __
 
b. On Friday, a new report was published that was described as follows by The Guardian: "More than half of US police killings are mislabelled or not reported, study finds." Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?
__The recent frequency of police shootings has produced strong negative feelings towards police officers and law enforcement. With the increase of participation in the "black lives matter" movement (which is extremely important), police officers are under attack for police killings.  I feel that this could definitely be the reason for mislabeling and under-reporting of police killings.  In order to prevent backlash from both inside the specific community and outside community, police killings are being mislabeled and underreported.  I find this extremely concerning, espcially if this is the reasoning.  Police officers need to be held accountable for their decisions, especially when their decisions lead to the slaying of innocent individuals.__

c. Regarding missing values in problem 4, do you see any? If so, do you think that's all that's missing from the data?
__In problem 4, I see that there is a category for race that is "unknown".  This variable is different from "other" because the race was either not reported or unknown by the data collectors. I also saw in gender and weapon that there was a category for "unknown" where data was missing or unreported.  Therefore, I do think that there is data that is missing from throughout the data set, not just in race.__ 

# Assignment 3 

Load the data.
```{r}
library(readr)
library(knitr)
dat.crime <- read_delim("crime_simple.txt", delim = "\t")
```

This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given. 

Here is the codebook:

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income


We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related. 


1. How many observations are there in the dataset? To what does each observation correspond?

```{r}
summary(dat.crime)
```

__There are 47 observations in this dataset.  They correspond to 47 US states.__

2. Draw a scatterplot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?

```{r, fig.width=6, fig.height=4}
reg.output.nc <- lm(formula = R ~ Ed, data = dat.crime)
plot(dat.crime$R,dat.crime$Ed, main = "Relationship between reported crime rate and average education for 47 states", xlab = "Reported Crime rate (# of offenses reported to police per million population)", ylab = "Average education (mean number of years of schooling for persons of age 25 or older)")
abline(reg.output.nc, col="skyblue")
```
```{r}
cor(dat.crime$R, dat.crime$Ed)
```
__Looking at the scatter plot, it does not appear that there is a linear relationship between the reported crime rate and average education.  In addition, the correlation between the two variables is only 0.3228349.  The blue line on the scatter plot shows that there is a positive correlation between the number of offenses reported to police (per million population) and the mean number of years of schooling for persons of ages 25+.  A correlation of 0.3228 corresponds to a weak, positive linear relationship.  Looking at the scatter plot and the correlation value, I would start to conclude that a linear model might not be the best way to to describe the relationship between these two variables.__


3. Regress reported crime rate (y) on average education (x) and call this linear model `crime.lm` and write the summary of the regression by using this code, which makes it look a little nicer `{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)`. (y~x: y regressed on x)
```{r}
crime.lm <-lm(formula=R~Ed, data = dat.crime)
summary(crime.lm)
``` 

```{r} 
kable(summary(crime.lm)$coef, digits = 2)
```

4. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)

```{r} 
plot(dat.crime$Ed, crime.lm$residuals, ylim = c(-50,50), main = "Residuals vs. x", xlab = "x, average years of education", ylab="Residuals" )
abline(h=0, lty="dashed")
```

__To check for error independence (which technically is not possible), we can look at the plot of residuals against x and check for patterns; there are not any patterns in the residuals vs. x graph, so we can assume that the independence assumption is met (assumption2).__  

```{r}
plot(crime.lm, which = 1)
```

__Looking at the residuals vs. fitted plot, the scatter plot smoother line that shows the average value of the residuals at each value of the fitted value, is relatively flat and close to the dashed line, which means that there is no noticeable non-linear trend to the residuals and they appear to be equally variable across the range of fitted values so we can assume that linearity assumption is met (assumption1).__


```{r}
plot(crime.lm, which=3)
```

__The red line is approximately horizontal and there aren't any significant trends which means our residuals have constant variance, so the equal variance  assumption is met (assumption3).__    
```{r}
plot(crime.lm, which = 5)
```
```{r}
plot(crime.lm, which=2)
```

__The Normal QQ plot shows that the tails do not lie along the line as well as they could, but most of the points fall along the line, so we can assume normality (assumption4).__

5. Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?
```{r}
summary(crime.lm)
```
__The estimated coefficient for the slope for this relationship is 1.1161. The standard error for slope is 0.4878 and the standard error for the intercept is 51.8104.  The p-value for the slope is 0.0269 and it is much larger for the intercept, 0.5996.  The p-value for the intercept is not significant.__

6. How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?
__The slope term in this model is saying that for every 1 year of average education increase, the number of offenses reported to police per million population goes up by by 1.1161.  However, we are not saying that there is a direct cause and effect in which when average education increases, the amount of offenses reported to police(per million population) has to also increase.__

7. Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?

__No, I cannot conclude that if individuals were to receive more education, then crime will be reported more often.  The p-values are not significant enough to reject the null hypothesis.  The null hypothesis states that the slope is zero, meaning that there is no relationship between average education and the number of offenses reported to the police.__

# Exam 2

### Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (sim.data.csv) onto that folder, and save your Exam 2.Rmd file in the same folder.

c. Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: **"Does having more funding in a police department lead to fewer incidents of police brutality?"**
d. Codebook:
- funds: How much funding the police department received in that year in millions of dollars.
- po.brut: How many incidents of police brutality were reported by the department that year.
- po.dept.code: Police department code

### Problem 1: EDA (10 points) 

Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.

```{r}
dat <- read.csv(file = 'sim.data.csv')

summary(dat)

hist(dat$funds, main="Histogram of Funding (millions of dollars)", xlab="Amount of funding received per year", ylab="Frequency", xlim=c(0,100), ylim=c(0,100),col="yellow")

hist(dat$po.brut, main="Incidents of police brutality", xlab="Number of Incidents", ylab="Frequency", xlim=c(0,50), ylim=c(0,100),col="red", breaks=12)

```

__This is a simulated data set about 200 police departments, each police department is coded and that is described by the variable po.brut.  This data set contains information about the funding that the department received (funds) in millions of dollars and the incidents of police brutality that were reported by the department in that year (po.brut).  Both the variables funds and po.brut are quantitative numerical variables that can be best visualized using quantitative data analysis plots such as histograms.  The average number of funds that police departments receive is $61.04 million dollars.  The histogram for funding appears to be have a relatively normal bell curve shape, with no right or left skew.  The average number of police brutality incidents is ~18, and the maximum number of incidents at any of the 200 police departments is 29 and the minimum amount of incidents at any of the departments is 0 incidents.  The histogram of police brutality incidents has more of a left skew, which is a better thing because those are the smaller values of incidents.__


### Problem 2: Linear regression (30 points)

a. Perform a simple linear regression to answer the question of interest. To do this, name your linear model "reg.output" and write the summary of the regression by using "summary(reg.output)". 

```{r}
reg.output <- lm(formula = po.brut~funds, data = dat)
summary(reg.output)
```

__answer__

b. Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.

__The estimated coefficient (slope) is -0.367099. This slope means that for every 1 million dollar increase in funding, the number of police brutality incidents on average would decrease by 0.367099. The standard error for the slope is 0.004496 and the standard error for the intercept is 0.282503.  The p-value of the slope is 2x10^-16, which we can see from the significant codes is very significant. Three stars means that the p-value is highly signficiant. The extremely small p-values express for both slope and intercept express that the relationship between funding and incidents is very statistically significant.__

c. Draw a scatterplot of po.brut (y-axis) and funds (x-axis). Right below your plot command, use abline to draw the fitted regression line, like this:
```{r, fig.width=4, fig.height=4}
# Remember to remove eval=FALSE!!
plot(dat$funds,dat$po.brut, main = "Relationship between Funding and Police Brutality Incidents", xlab = "Funding (in millions of dollars)", ylab="Incidents of Police Brutality" )
abline(reg.output, col = "red", lwd=2)
cor(dat$funds, dat$po.brut)
```
Does the line look like a good fit? Why or why not?

__Yes the line does look like a good fit for the data.  The relationship between  funding and incidents of police brutality looks linear tells me that a simple linear regression is a good model to fit this data set.  The scatterplot points appear to fall very well along the linear regression line.  The red line tells us that there is a strong negative correlation between police funding and police brutality incidents. In addition, the correlation between these two variables is -0.9854706, which also supports that there is a highly negative correlation between funding and police brutality incidents.__

d. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?
```{r}
#Linearity Assumption
plot(reg.output, which=1)
```
__Looking at the residuals vs. fitted plot, the scatter plot smoother line that shows the average value of the residuals at each value of the fitted value, is not very flat, which means that there could be a non-linear trend to the residual, and the residuals seem to be closely following this line, meaning there could be a trend in the  residuals and the linearity assumption is not necessarily met (assumption1).__
```{r}
#Independence Assumption
plot(dat$funds, reg.output$residuals, ylim=c(-15,15), main="Residuals vs. x", xlab="x, Funding in million of dollars", ylab="Residuals")
abline(h = 0, lty="dashed")
```
__To check for error independence (which technically is not possible), we can look at the plot of residuals against x and check for patterns; the residuals have a slight hill and follow a curved pattern, so we can assume that the independence assumption is not met (assumption2).__  
```{r}
#Equal Variance Assumption
plot(reg.output, which=3)
```
__The red line is clearly not flat and horizontal and there are significant trends on this plot, which means that the residuals/errors have non-constant variance and the equal variance assumption (assumption3) is not met.__   

```{r}
#Normal Population Assumption
plot(reg.output, which=2)
```

__The Normal QQ plot shows that the points fall along in the middle of the graph, but curve off in the extremities, the tails do not lie along the line as well as they could, especially the lower left corner of the plot, so I cannot assume that the normality assumption is met (assumption4).__

__Because the four assumptions are not met, but there seems to be a high negative linear correlation between the two variables, if I had more time I would before some sort of transformation to achieve linearity and fix my diagnostic plots.  Transforming the data using one of the possible transformation helps fix the diagnostics.  To achieve normality, I could determine which transformation to use by using the Box-Cox method. If I had more time to transform the data, I would run the code boxcox(reg.output) and look at the lambda value.  The lambda value would tell me which transformation to use fix my data.  With variables like wage and funding, it might be better to do log(funds), however this is just a hypothesis.  I would have to perform the actual transformation to determine which would work best.__ 

e. Answer the question of interest based on your analysis.

__The four assumptions of linearity are not met, however I do think that if I were to be able to perform one of the linear transformations, this data could be able to meet the assumptions of linearity.  Perhaps the linear model is not the best fit for this data, however, this would require much more analysis.  The correlation coefficient showed strong, negative linear correlation.  The R-squared value is extremely high as well.  The p-values are significant enough to reject the null hypothesis.  The null hypothesis states that the slope is zero, meaning that there is no relationship between funding (in millions) and number of police brutality incidents.  It is a little confusing because the the diagnostic plots look terrible, but the correlation, p-values, and scatter plot make me believe that there is a strong connection between the amount of funding in millions of dollars and incidents of police brutality.  I think that I can answer the question of interest (whether or not having more funding affects the number of incidents).  Based on my analysis, I can give a preliminary answer and say that there is a negative relationship between funding and the number of police brutality incidents, when there is more funding in a department, it is likely that the number of police brutality incidents would decrease (based on the slope and negative correlation).  However, more analysis should be done and data transformations should be done to really determine whether or not there is a strong enough relationship between these two variables. __

### Problem 3: Data ethics (10 points)

Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?

__The data set is simulated and does not include information on 200 real police departments.  This is a major major concern in interpreting the results from the data set.  How was this data set made?  It is important to know how a data set is collected, whether it is simulated or real.  It is important to know this in interpreting results.  However, because this is not a real data set, there are not any major ethical concerns with this exact data set.  If we were to have a data set with real information, it would be important to know if there was consent involved, if there was any bias in taking the sample, how the sample data was gathered (methodology).  There is a great deal to think about in concerning data ethics, but our sample is nice because it is simulated and does not have data on real people.  Lastly, a side note, I feel that it would be important to gather data on real police departments in order to interpret the results of this data set. In addition, it would be helpful to look at different variables as well to see if there are strong correlations between any other variables such as urban location and racial distribution.__

# Assignment 4 

```{r}
ggplot2::mpg
```
__This is the mpg data frame for the data set we are looking at.  It is a collection of variables (columns) and observations (rows).  mpg contains observations collected by the US Environmental Protection Agency on 38 models of car.__
```{r}
?mpg
```
__This code tells us more about the data set.__
```{r}
library(ggplot2)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
```
__This plot shows the relationship between engine size (displ) and fuel efficiency (hwy).  From this, we can see that there is a negative association between the two variables.  So this implies that when a car has a bigger engine,it uses more fuel. ggplot(data=mpg) creates an empty graph and you have to add more layers with the other code.__

### Aestetic Mappings 
There is a plot in which there are points that fall outside of the linear trend (in red).  Those cars have higher mileage that expected.  A third variable can be added to a scatter plot mapping it to an aesthetic.  Aesthetics include things like the size, the shape, or the color of your points.  Can use the world level to describe aesthetic properties.  
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))
```
__This new scatter plot added a third variable, class.  By looking at the color of the dot, we can determine the class of each vehicle. The points that do not follow the linear trend are 2 seater vehicles.  To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes().  Scaling is the process that assigns a unique value level of the aesthetic to a unique value.__

We can also map the variable class to show as different sizes instead of color.   
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, size = class))
```
__We got a warning, because mapping an unordered variable (class) to an ordered aesthetic(size) is not a good idea.__

We can also map class to the alpha esthetic, which controls the transparency of the points.
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))
```
__When plotting this, I got a warning signal. Warning: Using alpha for a discrete variable is not advised. ggplot2 will only use six shapes at a time. By default, additional groups will go unplotted when you use the shape aesthetic.  The x and y are aesthetics itself, the axis act as a legend.__

We can also set the aesthetic properties of the graph ourselves, for example, by making all of the points blue. 
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
```
__This does not tell anything about the variables, but it allows you to change the appearance of the plot. When you manually changing the aesthetics, they go on the outside of aes(). You can change the color, the size of the points, and the shape of a point (by using a number).__

### Common Problems 
One common issue with ggplot2 is putting the + in the wrong place.  The + must come at the end of the line, not the start.

### Facets 
You can split your plot into facets, which are subplots that each display one subset of the data. To facet your plot by a single variable, you can use facet_wrap(). The first argument has to be a formula, which is ~ followed by the variable name.
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```
You can also plot on the combination of two variables and add the code facet_grid().  This is also contains a formula: variable 1 ~ variable 2. 
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
```
### Geometric Objects
Geoms are the geometrical object that a plot uses to represent data. For example, bar charts use bar geoms.  You can change the type of geom being used in your plot. 
````{r}

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))
```
__Geom_point uses points and geom_smooth uses smooth representation.__

You can change the type of line that is used in a geom_smooth graph. 
```{r}
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
```
__Geom_smooth() separates the cars based on their drv value (drivetrain).  4=four wheel drive, f = front wheel drive, and r = rear wheel drive.__

Many geoms use a single geometric display to display multiple rows of data.  You also can se the group aesthetic to a categorical variable to draw multiple objects.
```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
              
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))
    
ggplot(data = mpg) +
  geom_smooth(
    mapping = aes(x = displ, y = hwy, color = drv),
    show.legend = FALSE
  )
```
You also can display multiple geoms in the same plot by adding different geo functions to ggplot.  Below, we are adding a point geom and smooth geom to our plot.  
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
```
However, since you are looking at the same variables, there is some duplication in the code.  You can simplify the code by writing your code like this, instead of the code above: 
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()
```
The same plot is produced, just with a simpler code. 

You also can display different aesthetics in different layers.  For example, in the plot below, we are displaying the class as different colors in the point geom layer. 
```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth()
```
### Statistical Transformation 
Bar charts 
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))
```
This is a simple barplot that shows the cut of the diamond and the count. It is using counts from something called stat.  You can usually use geoms and stats interchangeably.  The same plot can be produced by changing the code slightly: 
```{r}
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
```
We also can display a bar chart of proportion, rather than count, using the following code:  
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
```
We are manipulating our data using this stat function, and it is important to know what we are doing to it.  We can use stat_summary() to summarize the y values for each x value: 
```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.min = min,
    fun.max = max,
    fun = median
  )
```

### Position adjustments 
We can also color a bar chart by using the code fill: 
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))
```
__The code colour is telling R to only have the edges of the bars colored, but the code fill is telling R to produce a plot in which the entire bar is colored.__

You can add another variable to make a stacked barplot.  In this case, every colored rectangle represents a combination of cut and clarity: 
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
```
If you do not want a stacked barplot, but still want to look at multiple variables you can use the code identity, dodge, or fill.  
Identity is not super useful for bar plots.  
```{r}
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")
```
__The bars overlap, so to see the overlap, we have to make the bars either slightly transparent (set alpha to a low number) or completely transparent (fill = NA).__

position = "fill" makes each set of stacked bars the same hieght and you can look at proportions. 
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
```
position = "dodge" places overlapping objects directly beside one another and you are able to better compare individual values. 
```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
```

__For scatterplots, there is another adjustment that can be made to the plot to avoid overplotting. We want to be able to see where the mass of the data is and the position = "jitter".__
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
```

### Coordinate Systems 
We can plot coordinate systems using ggplot.  The default is a Cartesian coordinate system.  One function that we can use is coord_flip() which switches the x and y axes. 
```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
```

coord_quickmap() sets the aspect ratio correctly for maps.  

```{r}
nz <- map_data("nz")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
```
The function coord_polar() uses polar coordinates. 
```{r}
bar <- ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_flip()
bar + coord_polar()
```

### Graphics for Communication (Chapter 28)
The best place to start when turning an exploratory graphic into a expository graphic is with good labels.  Add labels with the labs() function.  The examplle below adds a plot title. 

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(title = "Fuel efficiency generally decreases with engine size")
```
You can also add subtitles using code subtitle and the code caption to add text at the bottom right of the plot: 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov"
  )
```
You can change the x and y labels too, use the code labs() to change the axis and legend titles: 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    colour = "Car type"
  )
```

### Annotations 
You can also label individual observations or groups of observations. geom_text() is similar to geom_point(), but it has an additional aesthetic: label. This code makes it possible to add textual labels to plots.  
```{r}
library('dplyr') 
library(ggplot2)
```
Needed to download the piping option. 

```{r}
best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_text(aes(label = model), data = best_in_class)
```
geom_label() which draws a rectangle behind the text, making it easier to read our plot. 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)
```

There is overlap in the labels, so we can use the ggrepel package, which adjusts labels so they do not overlap. 

```{r}
library('ggrepel')
```
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_point(size = 3, shape = 1, data = best_in_class) +
  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)
```
I had to download the package, ggrepel.  You can also replace the legend with labels placed on the plot, this is show in the below code and plot: 
```{r}
class_avg <- mpg %>%
  group_by(class) %>%
  summarise(
    displ = median(displ),
    hwy = median(hwy)
  )
#> `summarise()` ungrouping output (override with `.groups` argument)

ggplot(mpg, aes(displ, hwy, colour = class)) +
  ggrepel::geom_label_repel(aes(label = class),
    data = class_avg,
    size = 6,
    label.size = 0,
    segment.color = NA
  ) +
  geom_point() +
  theme(legend.position = "none")
```
You can also add a single label to the plot, in the corner of the plot.  You can create a new data frame using the code (summarise()) to compute the max and min values of the variables x and y.  
```{r}
label <- mpg %>%
  summarise(
    displ = max(displ),
    hwy = max(hwy),
    label = "Increasing engine size is \nrelated to decreasing fuel economy."
  )

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
```
```{r}
label <- tibble(
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
```
Putting the text in the exact corner of the plot - use +Inf and -Inf: 
```{r}
label <- tibble(
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
```
There are other ways to annotate your plots: 
- geom_hline() and geom_vline() to add reference lines. 
- geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax.
-geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.

### Scales 
It is important to adjust the scales if necessary, to make your plots look better. You can adjust the axis ticks and legend keys. 
The code breaks controls the position of the ticks. Labels controls the text label associated with the tick. 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_y_continuous(breaks = seq(15, 40, by = 5))
```
You can remove the labels from an axis, you use labels = NULL. 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL)
```
__Axis and legends are known as guides as well.  Axes are used for x and y aesthetics.__

You can use breaks when you have not that many data points and want to highlight where observations occur: 
```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")
```

You can change the position of the legends as well.  To control where the legend is, you can use theme().  Theme code controls non-data parts of a plot.  theme(legend.position = ? ) controls where the legend is drawn.  If you put legend.position = none, the legend is removed.   
```{r}
base <- ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

base + theme(legend.position = "left")
base + theme(legend.position = "top")
base + theme(legend.position = "bottom")
base + theme(legend.position = "right") # the default
base + theme(legend.position = "none") 

```

In addition, you can make other changes to a legend by using guides() along with guide_legend() and guide_colourbar(). 
-nrow: controls the number of rows the legend uses 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  theme(legend.position = "bottom") +
  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))
#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'
```

### Replacing the scale 
```{r}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d()

ggplot(diamonds, aes(log10(carat), log10(price))) +
  geom_bin2d()
```
We want to change this so that the axes are now labeled with the  transformed values.  We can change this so that the transformation is not in the aesthetic mapping, but instead in the scale: 
```{r}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d() + 
  scale_x_log10() + 
  scale_y_log10()
```

You can change the color that is being used as well. 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv))

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv)) +
  scale_colour_brewer(palette = "Set1")
```
 
 To make it even more distinguishable between the types of points, you can use shapes.  The following code changes the shape of the dots: 
```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv, shape = drv)) +
  scale_colour_brewer(palette = "Set1")
```
```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, colour = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))
```
If you have a predefined mapping between values and colors you can use the code scale_colour_manual() to code for specific colors. 

### Zooming 
There are three ways to control the plot limits: adjust which data is being plotted on a plot, set the limits in each of the scales, setting xlim and ylime in coord_cartesian(). You can use that last code to zoom in on a region of the plot and focus on specific data. 
```{r}
ggplot(mpg, mapping = aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth() +
  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))

mpg %>%
  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>%
  ggplot(aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth()
```

You can set limits on individual scales, which is basically like subsetting the data.  Most useful if you want to expand the limits. 
```{r}
suv <- mpg %>% filter(class == "suv")
compact <- mpg %>% filter(class == "compact")

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point()

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point()
```

For example, the scales are all different in those above plots.  To overcome this issue, you can share scales across multiple plots with the following code: 
```{r}
x_scale <- scale_x_continuous(limits = range(mpg$displ))
y_scale <- scale_y_continuous(limits = range(mpg$hwy))
col_scale <- scale_colour_discrete(limits = unique(mpg$drv))

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale
```
 
### Themes 
Customize nondata elements of your plot using themes. 

### Saving your plots 
You can save your plots out of R and into a final write up using the codes ggsave() and knitr.ggsave().  These codes save the most recent plot. 

# CRIM250 Final Paper 
Michaela Rieser, Rachael Villari, Sara Whitelaw 
December 8, 2021
### Crime Distributions at Ivy League Institution 

### 1 INTRODUCTION 
  At the University level, it is important that crime trends are observed in relation to the external factors that may have an effect. Ivy League universities, being private institutions that receive billions of dollars in endowments, should have ample measures in place to control crime rates on campus to maintain the safety of students. In this report, we will observe the total crime trends between the University of Pennsylvania, Yale University, and Columbia University and then take a more specific look at sexual offense trends. These trends will be compared with external research that analyzes police funding and the implementation of Title IX. Ultimately, it is important that more causal research is conducted in this area to observe whether it is beneficial to have privately funded university police departments and whether to bolster Title IX power. The data used in this report contain the total crime data from 2001-2017 for the three Ivy League institutions mentioned above. 

### 2 EXPLORATORY DATA ANALYSIS 
__2.1  DATA DESCRIPTION__
  The selected dataset was compiled data from Jacob Kaplan on total crimes reported at Universities (1).  All of the data comes from the Department of Education Office of Postsecondary Education, which collects crime data from colleges. The following variables were the focus of the analysis: total number of aggravated assault, total number of arson, total number of motor vehicle theft, total number of non-negligent manslaughter, total number of negligent manslaughter, total number of sex offenses. The variables total number of on campus sex crimes and total number of off campus sex crimes were explored as well.  All datasets used for this analysis individually contain 17 years of data with 1039 observations. 

__2.2 MISSING VALUES__ 
  There are many missing values because, different from the value of 0, there are crime categories that do not have any reported crime data for certain years. For example, looking at the University of Pennsylvania data, there is no data for off-campus rape crimes until 2014.  This will affect our data analysis, because the crime statistics may seem lower than they actually are.

__2.3 UNIVERSITY LOCATIONS__ 
  The three universities are located in urban settings. University of Pennsylvania is located in Philadelphia, PA (2019 pop. = 1.58 million) (2).  Columbia University is located in New York City, NY (2019 pop. = 8.33 million) (2). Yale University is in New Haven, CT (2019 pop. = 130,250)(2).  All population data was retrieved from the US census population estimates. 

__2.4 CAMPUS POLICE FORCE__ 
	The University of Pennsylvanias Division of Public Safety (DPS) comprises 180 personnel and 121 of these are sworn-in University of Pennsylvania police officers (3). The Yale Police Department (YPD) comprises 93 sworn-in police officerS (4). Due to New York regulations, Columbia University has 147 full-time security officers and no sworn-in officers (5).  These security officers cannot carry firearms or arrest individuals, but they can detain suspected criminals on the University property. However, Columbia University is in the 26th precinct of the NYPD (6).   

__2.5 TOTAL CRIMES PER YEAR__
```{r}
datupenn <- read.csv(file = 'upenndata.csv')
plot(datupenn$year, datupenn$crimes_total_total, type = "b", main = "Scatterplot of total number of reported crimes at UPenn from 2001-2017", xlab = "Year", ylab = "Number of offenses reported to police", cex.axis=0.75, ylim=c(0,300))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```
```{r}
datcolumbia <- read.csv(file = 'columbiadata.csv')
plot(datcolumbia$year, datcolumbia$crimes_total_total, type = "b", main = "Scatterplot of total number of reported crimes at Columbia from 2001-2017", xlab = "Year", ylab = "Number of offenses reported to police", cex.axis=0.75, ylim=c(0,300))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
datyale <- read.csv(file = 'yaledata.csv')
plot(datyale$year, datyale$crimes_total_total, type = "b", main = "Scatterplot of total number of reported crimes at Yale from 2001-2017", xlab = "Year", ylab = "Number of offenses reported to police", cex.axis=0.75, ylim=c(0,300))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

__2.6 AVERAGE OF TOTAL SEX OFFENSES PER YEAR__
```{r}
plot(datupenn$year, datupenn$crimes_total_sex_offenses_total, type = "b", main = "Scatterplot of total # of reported sex crimes at UPenn from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.main=0.75, cex.axis=0.75, ylim=c(0,35))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
plot(datcolumbia$year, datcolumbia$crimes_total_sex_offenses_total, type = "b", main = "Scatterplot of total # of reported sex crimes at Columbia from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.main=0.75, cex.axis=0.75, ylim=c(0,35))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
plot(datyale$year, datyale$crimes_total_sex_offenses_total, type = "b", main = "Scatterplot of total # of reported sex crimes at Yale from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.main=0.75, cex.axis=0.75, ylim=c(0,35))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

__2.7 DIFFERENCES BY ON/OFF CAMPUS__
__2.7.1 ON CAMPUS SEX OFFENSES__
```{r}
plot(datupenn$year, datupenn$crimes_on_campus_sex_offenses_total, type = "b", main = "Scatterplot of total # of reported on campus sex crimes at UPenn from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.axis=0.75, cex.main=0.75, ylim=c(0,35))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
plot(datcolumbia$year, datcolumbia$crimes_on_campus_sex_offenses_total, type = "b", main = "Scatterplot of total # of reported on campus sex crimes at Columbia from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.axis=0.75, cex.main=0.75, ylim=c(0,35))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
plot(datyale$year, datyale$crimes_on_campus_sex_offenses_total, type = "b", main = "Scatterplot of total # of reported on campus sex crimes at Yale from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.axis=0.75, cex.main=0.75, ylim=c(0,35))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

__2.7.2 OFF CAMPUS SEX OFFENSES__
```{r}
plot(datupenn$year, datupenn$crimes_noncampus_sex_offenses_total , type = "b", main = "Scatterplot of total # of reported off campus sex crimes at UPenn from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.axis=0.75, cex.main=0.75, ylim=c(0,5))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
plot(datcolumbia$year, datcolumbia$crimes_noncampus_sex_offenses_total , type = "b", main = "Scatterplot of total # of reported off campus sex crimes at Columbia from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.axis=0.75, cex.main=0.75, ylim=c(0,5))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

```{r}
plot(datyale$year, datyale$crimes_noncampus_sex_offenses_total , type = "b", main = "Scatterplot of total # of reported off campus sex crimes at Yale from 2001-2017", xlab = "Year", ylab = "Number of sex offenses reported to police", cex.axis=0.75, cex.main=0.75, ylim=c(0,5))
axis(1, at=seq(2001,2017,1), cex.axis=0.75)
```

__2.8 DIFFERENCES IN DIFFERENT CRIME TYPES___
```{r}
vars<- c("year","crimes_total_aggravated_assault", "crimes_total_arson","crimes_total_motor_vehicle_theft","crimes_total_murder_non_negligent_manslaughter", "crimes_total_negligent_manslaughter", "crimes_total_sex_offenses_total" )
newupenndata <- datupenn[,vars]
```

```{r}
names(newupenndata)[names(newupenndata)=="crimes_total_aggravated_assault"] <- "Aggravated Assault"
names(newupenndata)[names(newupenndata)=="crimes_total_arson"] <- "Arson"
names(newupenndata)[names(newupenndata)=="crimes_total_motor_vehicle_theft"] <- "Motor Vehicle Theft"
names(newupenndata)[names(newupenndata)=="crimes_total_murder_non_negligent_manslaughter"] <- "Non-Negligent Manslaughter"
names(newupenndata)[names(newupenndata)=="crimes_total_negligent_manslaughter"] <- "Negligent Manslaughter"
names(newupenndata)[names(newupenndata)=="crimes_total_sex_offenses_total"] <- "Sex Offenses"
```

```{r}
library(kableExtra)
```

```{r}
kbl(newupenndata)
newupenndata %>%
  kbl() %>%
  kable_styling()
```

```{r}
Totals <- c(213,14,246,6,0,215)
Crime <- c('Aggravated_Assault', 'Arson', 'Motor_Theft', 'Non_Negligent', 'Negligent', 'sex_offenses')
df <- data.frame(Totals, Crime)

barplot(Totals~Crime,data=df, main="Total Number of Different Crimes Reported at Penn 2001-2017", xlab="Crime Type", ylab="Total",ylim=c(0,250), cex.names=.5)
```

```{r}
vars<- c("year","crimes_total_aggravated_assault", "crimes_total_arson","crimes_total_motor_vehicle_theft","crimes_total_murder_non_negligent_manslaughter", "crimes_total_negligent_manslaughter", "crimes_total_sex_offenses_total" )
newcolumbiadata <- datcolumbia[,vars]
```

```{r}
names(newcolumbiadata)[names(newcolumbiadata)=="crimes_total_aggravated_assault"] <- "Aggravated Assault"
names(newcolumbiadata)[names(newcolumbiadata)=="crimes_total_arson"] <- "Arson"
names(newcolumbiadata)[names(newcolumbiadata)=="crimes_total_motor_vehicle_theft"] <- "Motor Vehicle Theft"
names(newcolumbiadata)[names(newcolumbiadata)=="crimes_total_murder_non_negligent_manslaughter"] <- "Non-Negligent Manslaughter"
names(newcolumbiadata)[names(newcolumbiadata)=="crimes_total_negligent_manslaughter"] <- "Negligent Manslaughter"
names(newcolumbiadata)[names(newcolumbiadata)=="crimes_total_sex_offenses_total"] <- "Sex Offenses"
```

```{r}
kbl(newcolumbiadata)
newcolumbiadata %>%
  kbl() %>%
  kable_styling()
```

```{r}
Totals <- c(152,7,62,5,0,196)
Crime <- c('Aggravated_Assault', 'Arson', 'Motor_Theft', 'Non_Negligent', 'Negligent', 'sex_offenses')
df <- data.frame(Totals, Crime)

barplot(Totals~Crime,data=df, main="Total Number of Different Crimes Reported at Columbia 2001-2017", xlab="Crime Type", ylab="Total",ylim=c(0,250), cex.names=.5)
```

```{r}
vars<- c("year","crimes_total_aggravated_assault", "crimes_total_arson","crimes_total_motor_vehicle_theft","crimes_total_murder_non_negligent_manslaughter", "crimes_total_negligent_manslaughter", "crimes_total_sex_offenses_total" )
newyaledata <- datyale[,vars]
```

```{r}
names(newyaledata)[names(newyaledata)=="crimes_total_aggravated_assault"] <- "Aggravated Assault"
names(newyaledata)[names(newyaledata)=="crimes_total_arson"] <- "Arson"
names(newyaledata)[names(newyaledata)=="crimes_total_motor_vehicle_theft"] <- "Motor Vehicle Theft"
names(newyaledata)[names(newyaledata)=="crimes_total_murder_non_negligent_manslaughter"] <- "Non-Negligent Manslaughter"
names(newyaledata)[names(newyaledata)=="crimes_total_negligent_manslaughter"] <- "Negligent Manslaughter"
names(newyaledata)[names(newyaledata)=="crimes_total_sex_offenses_total"] <- "Sex Offenses"
```

```{r}
kbl(newyaledata)
newyaledata %>%
  kbl() %>%
  kable_styling()
```

```{r}
Totals <- c(56,10,306,2,0,246)
Crime <- c('Aggravated_Assault', 'Arson', 'Motor_Theft', 'Non_Negligent', 'Negligent', 'sex_offenses')
df <- data.frame(Totals, Crime)

barplot(Totals~Crime,data=df, main="Total Number of Different Crimes Reported at Yale 2001-2017", xlab="Crime Type", ylab="Total",ylim=c(0,325), cex.names=.5)
```

### 3 CAUSAL ANALYSIS
__3.1 CAUSAL PARAMETER OF INTEREST__
  The causal parameters of interest are the size and funding of the police forces of the three Ivy League institutions: University of Pennsylvania, Yale University, and Columbia University. Additionally, we note the effect of Title IX guidelines on the sexual offense trends. While Columbia has 147 security officers rather than a private police force, Columbia benefits from the NYPD as they are located in the 26th precinct. Due to the size and power of the NYPD, Columbias total crime trend could be more reflective of the crime trends in the surrounding New York area. UPenn has a private police force of 121 sworn officers, which receives 27 million dollars of funding. UPenns negative crime trend could be a result of the robust size and funding of the police force, but without causal analysis and true experiments that include randomization, it is unclear whether this external factor has a relevant impact. Finally, on-campus sexual offense trends could be influenced by the causal parameter of Title IX guidelines and their implementation in university proceedings surrounding sexual offense cases. Title IX allows increased protections and assurance of investigation for on-campus students, which could possibly explain the high reporting rates for sexual offenses. Conversely, Title IX guidelines limit protections in off campus offenses noted in our discussion section (7). 
	It is important to note that these causal parameters can only be hypothesized as we do not have sufficient data to analyze whether they play an influential role in our EDA of the crime distributions of the three Ivy Leagues explored. 
	
__3.2 PROPOSED CAUSAL DAG__
  The proposed causal DAG consists of two actions, the outcomes of the actions, and the confounding variables that may impact both the actions and the outcomes. In our case of understanding the crime distribution of the Columbia, Yale, and UPenn, the actions that influence the crime reporting rates are the strength and presence of the campus funded police force and Title IX when regarding the reporting of sex offenses. The expected outcomes of higher funding and strong on campus police force would seemingly decrease total crime rates whereas decreased funding and smaller police force would result in higher crime. Title IXs on-campus guidelines would result in higher crime reporting of sex offenses due to increased victim protection whereas their off campus guidelines would result in decreased reporting. However, the confounding variables that may impact both these actions and outcomes are reporting bias, in that there may be failure to report by police offices as well as failure to report by victim (sex assault), and the danger of the location of the school. 
    
### 4 Conclusion 
  We wanted to determine whether 1) There was a significant difference in total crime rates and sexual offenses between three different urban, Ivy League institutions, and 2) if police size and funding, and Title IX guidelines influenced this relationship. Due to a lack of sufficient data on which we could perform linear regression to explore the correlation between these two actions (police funding and Title IX guidelines), it is unclear whether they have an impact on the total crime trends and sexual offense rates at any of the Ivy Leagues we analyzed. However, overall, we found that the University of Pennsylvania had the highest total number of reported crimes from 2001-2017, with a total of 2069 crimes compared to Yale (1991) and Columbia (1589). It is important to note that Columbia had two years of missing data, which could change how its crime rates actually compare to the other universities. Yale University had the highest number of reported sex offenses (246) compared to Columbia University (196)  and University of Pennsylvania (216). We find this especially interesting because UPenn has the highest crime rate, despite having the largest number of on-campus sworn-in officers. Additionally, it is intriguing that Yale University is located in the smallest city (population =130,250) of the three schools analyzed, but still has the highest number of reported sex offenses. Yale University also has the lowest police force funding ($10 million), despite having the largest endowment ($42.3 billion) of the three Ivies (2) (8) (9). We also found that almost all sex offenses reported to the respective campus police forces occurred on campus, rather than off campus. It is possible that this is because off-campus sex offenses are reported to the city police forces. In terms of Title IX providing explanation for the stark difference between on campus and off campus sexual offense trends, new Title IX guidelines do not require universities to investigate events that occur off campus. Therefore, there could ultimately be higher off campus sexual offense trends due to lack of investigation by universities. 
  Our results have some limitations. 
  The Columbia University data set was missing crime data for 2002 and 2004.  The data sets that we used were subject to reporting bias and missing data.  Failure to report by police officers and failure to report by victim, which is especially prevalent with sexual assault cases, has an effect on the total number of crimes reported, as well as the total number of sexual offenses reported.  In addition, due to limited data, we only used scatter and box plots to compare the crime distribution and analyze the relationship between police force funding, Title IX enactment, and crime distribution.  The use of linear regression would allow for strong analysis between these variables.     

### Sources 
1. Kaplan, J. (n.d.). Crimes On University Campuses. Data | Jacob Kaplan. Retrieved December 7, 2021, from https://jacobdkaplan.com/data.html. 
2. Bureau, U. S. C. (2021, October 8). American Community survey 5-year data (2009-2019). Census.gov. Retrieved December 7, 2021, from https://www.census.gov/data/developers/data-sets/acs-5year.html. 
3. Pennsylvania, U. of. (n.d.). About our division. Division of Public Safety. Retrieved December 7, 2021, from https://www.publicsafety.upenn.edu/about/. 
4. Yale University. (2020, June). Yale Police Department. It's Your Yale. Retrieved December 7, 2021, from https://your.yale.edu/community/public-safety/yale-police-department. 
5. Heinzerling, K. (2017, October 8). With 120 officers, Penn has the largest private police force in Pennsylvania. The Daily Pennsylvanian. Retrieved December 7, 2021, from https://www.thedp.com/article/2017/10/with-120-officers-penn-has-the-largest-private-police-force-in-pennsylvania. 
6. 26th precinct. 26 Precinct - NYPD. (n.d.). Retrieved December 7, 2021, from https://www1.nyc.gov/site/nypd/bureaus/patrol/precincts/26th-precinct.page. 
7. Feldman, S. (2018, October 8). Yale and Title IX: A Complicated History and an Unclear Future. The Politic. Retrieved December 7, 2021, from https://thepolitic.org/yale-and-title-ix-a-complicated-history-and-an-unclear-future/.  
8. Why: Black students for disarmament at Yale. bsdy. (n.d.). Retrieved December 7, 2021, from https://www.defundypd.com/why. 
9. Yale Investments Office. (n.d.). Retrieved December 07, 2021, from https://investments.yale.edu/







